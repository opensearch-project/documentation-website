---
layout: default
title: Semantic
nav_order: 20
parent: Supported field types
---

# Semantic field type
**Introduced 3.1**
{: .label .label-purple }

The `semantic` field type is a high-level abstraction that simplifies neural search setup in OpenSearch. It can wrap a variety of field types, including all string and binary fields. The `semantic` field type automatically enables semantic indexing and querying based on the configured machine learning (ML) model.

**PREREQUISITE**<br>
Before using the `semantic` field type, you must configure either a local ML model hosted on your OpenSearch cluster or an externally hosted model connected to your OpenSearch cluster. For more information about local models, see [Using ML models within OpenSearch]({{site.url}}{{site.baseurl}}/ml-commons-plugin/using-ml-models/). For more information about externally hosted models, see [Connecting to externally hosted models]({{site.url}}{{site.baseurl}}/ml-commons-plugin/remote-models/index/).
{: .note}

## Example: Dense embedding model

Once you configure a model, you can use it to create an index with a `semantic` field. This example assumes that you have configured a dense embedding model with the ID `n17yX5cBsaYnPfyOzmQU` in your cluster:

```json
PUT /my-nlp-index
{
  "settings": {
    "index": {
      "knn": true
    }
  },
  "mappings": {
    "properties": {
      "passage": {
        "type": "semantic",
        "model_id": "n17yX5cBsaYnPfyOzmQU"
      }
    }
  }
}
```
{% include copy-curl.html %}

After creating the index, you can retrieve its mapping to verify that a `passage_semantic_info` field was automatically created. The `passage_semantic_info` field contains a `knn_vector` subfield for storing the dense embedding and additional metadata fields for capturing information such as the model ID, model name, and model type:

```json
GET /my-nlp-index/_mapping
{
  "my-nlp-index": {
    "mappings": {
      "properties": {
        "passage": {
          "type": "semantic",
          "model_id": "n17yX5cBsaYnPfyOzmQU",
          "raw_field_type": "text"
        },
        "passage_semantic_info": {
          "properties": {
            "embedding": {
              "type": "knn_vector",
              "dimension": 384,
              "method": {
                "engine": "faiss",
                "space_type": "l2",
                "name": "hnsw",
                "parameters": {}
              }
            },
            "model": {
              "properties": {
                "id": {
                  "type": "text",
                  "index": false
                },
                "name": {
                  "type": "text",
                  "index": false
                },
                "type": {
                  "type": "text",
                  "index": false
                }
              }
            }
          }
        }
      }
    }
  }
}
```
{% include copy-curl.html %}

The `dimension` and `space_type` of the `knn_vector` field are determined by the ML model configuration. For [pretrained dense models](ml-commons-plugin/pretrained-models/#sentence-transformers), this information is included in the default model configuration. For externally hosted dense embedding models, you must explicitly define the `dimension` and `space_type` in the model configuration before using the model with a `semantic` field.

The autogenerated `knn_vector` subfield supports additional settings that are not currently configurable in the `semantic` field. For more information, see [Limitations](#limitations).
{: .note}

## Example: Sparse encoding model

Once you configure a model, you can use it to create an index with a `semantic` field. This example assumes that you have configured a sparse encoding model with the ID `n17yX5cBsaYnPfyOzmQU` in your cluster:

```json
PUT /my-nlp-index
{
  "mappings": {
    "properties": {
      "passage": {
        "type": "semantic",
        "model_id": "nF7yX5cBsaYnPfyOq2SG"
      }
    }
  }
}
```
{% include copy-curl.html %}

After creating the index, you can retrieve its mapping to verify that a `rank_features` field was automatically created:

```json
GET /my-nlp-index/_mapping
{
  "my-nlp-index": {
    "mappings": {
      "properties": {
        "passage": {
          "type": "semantic",
          "model_id": "nF7yX5cBsaYnPfyOq2SG",
          "raw_field_type": "text"
        },
        "passage_semantic_info": {
          "properties": {
            "embedding": {
              "type": "rank_features"
            },
            "model": {
              "properties": {
                "id": {
                  "type": "text",
                  "index": false
                },
                "name": {
                  "type": "text",
                  "index": false
                },
                "type": {
                  "type": "text",
                  "index": false
                }
              }
            }
          }
        }
      }
    }
  }
}
```
{% include copy-curl.html %}

## Parameters

The `semantic` field type supports the following parameters.

| Parameter                        | Data type                | Updatable | Required/Optional | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
|----------------------------------|--------------------------|-----------|----------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `type`                           | String                   | false     | Required  | Must be set to `semantic`.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
| `raw_field_type`                 | String                   | false     | Optional  | The underlying field type wrapped by the `semantic` field. The raw input is stored as this type at the path of the semantic field, allowing it to behave like a standard field of that type. Valid values are `text`, `keyword`, `match_only_text`, `wildcard`, `token_count`, and `binary`. Default is `text`. You can use any parameters supported by the underlying field type; those parameters function as expected.                                                                                                                        |
| `model_id`                       | String                   | true      | Required  | The ID of the ML model used to generate embeddings from field values during indexing and from query input during search.                                                                                                                                                                                                                                                                                                                                                                                                                         |
| `search_model_id`                | String                   | true      | Optional  | The ID of the ML model used specifically for query-time embedding generation. If not specified, the `model_id` is used. Cannot be specified together with `semantic_field_search_analyzer`.                                                                                                                                                                                                                                                                                                                                                      |
| `semantic_info_field_name`       | String                   | false     | Optional  | A custom name for the internal metadata field that stores the embedding and model information. By default, this field name is derived by appending `_semantic_info` to the semantic field name.                                                                                                                                                                                                                                                                                                                                                  |
| `chunking`                       | Boolean or Array of Maps | false     | Optional  | Enables chunking of long-form text during ingestion. Set to true to use a default fixed token-length strategy, or specify a list of strategy objects to apply multiple chunking algorithms in sequence. See [Text chunking](#text-chunking).                                                                                                                                                                                                                                                                                                     |
| `semantic_field_search_analyzer` | String                   | true      | Optional  | Specifies an analyzer for tokenizing the query input when using a sparse model. Valid values are `standard`, `bert-uncased`, and `mbert-uncased`. Cannot be used together with `search_model_id`. For more information, see [Analyzers]({{site.url}}{{site.baseurl}}/analyzers/supported-analyzers/).                                                                                                                                                                                                                                            |
| `dense_embedding_config`         | Map                      | false     | Optional  | Defines custom settings for the underlying knn_vector field used when a semantic field is backed by a dense embedding model. This allows fine-grained control over vector indexing behavior, similarity function, and engine parameters. If omitted, OpenSearch applies default settings based on the model's embedding dimension and engine defaults. See [Dense Embedding Config](#dense-embedding-config).                                                                                                                                    |
| `sparse_encoding_config`         | Map                      | false     | Optional  | Configures how sparse vectors are encoded for the semantic field when using a sparse model. Supports all pruning strategies available in the [sparse_encoding processor]({{site.url}}{{site.baseurl}}/ingest-pipelines/processors/sparse-encoding/#pruning-sparse-vectors). If omitted, a default pruning strategy is applied using `max_ratio` with a prune ratio of `0.1`. This helps reduce noise and index size while preserving the most informative dimensions of the sparse vector. See [Sparse Encoding Config](#sparse-encoding-config) |


## Text chunking

By default, text chunking is disabled for `semantic` fields. This is because enabling chunking requires storing each chunk's embedding in a nested object, which can increase search latency. Searching nested objects requires joining child documents to their parent, along with additional scoring and aggregation logic. The more matching child documents there are, the higher the potential latency.

If you're working with long-form text and want to improve search relevance, you can enable chunking through the `chunking` parameter for the `semantic` field.

### Basic Chunking (Boolean true)

To enable default chunking behavior (fixed token-length with defaults):

```json
PUT /my-nlp-index
{
  "mappings": {
    "properties": {
      "passage": {
        "type": "semantic",
        "model_id": "nF7yX5cBsaYnPfyOq2SG",
        "chunking": true
      }
    }
  }
}
```
{% include copy-curl.html %}

This is equivalent to:
```json
PUT /my-nlp-index
{
  "mappings": {
    "properties": {
      "passage": {
        "type": "semantic",
        "model_id": "nF7yX5cBsaYnPfyOq2SG",
        "chunking": [
          {
            "algorithm": "fixed_token_length"
          }
        ]
      }
    }
  }
}
```

Chunking is performed using the [fixed token length algorithm]({{site.url}}{{site.baseurl}}/ingest-pipelines/processors/text-chunking/#the-fixed-token-length-algorithm). 

### Advanced Chunking (List of Strategies)

You can specify chunking as a list of chunking strategies, where each strategy is applied in sequence to the semantic field. Each item in the list must specify an algorithm and its parameters.

Example: Apply Delimiter, Then Fixed Token Chunking

```json
PUT /my-nlp-index
{
  "mappings": {
    "properties": {
      "passage": {
        "type": "semantic",
        "model_id": "nF7yX5cBsaYnPfyOq2SG",
        "chunking": [
          {
            "algorithm": "delimiter",
            "parameters": {
              "delimiter": "\n\n"
            }
          },
          {
            "algorithm": "fixed_token_length",
            "parameters": {
              "token_limit": 128,
              "overlap_rate": 0.2
            }
          }
        ]
      }
    }
  }
}
```

In the example above:

1. The field is first split using paragraph delimiters (\n\n)
2. Each resulting chunk is further split into fixed-size token windows

This gives you more control over how input text is segmented and ensures embeddings better reflect natural language boundaries.

### Supported Algorithms

All algorithms supported by the [text chunking ingest processor]({{site.url}}{{site.baseurl}}/ingest-pipelines/processors/text-chunking) are available here.

| Algorithm            | Description                                     |
|----------------------| ----------------------------------------------- |
| `fixed_token_length` | Splits input by number of tokens                |
| `delimiter`          | Splits input using a character/string delimiter |
| `fixed_char_length`  | Splits input by character count                 |


## Dense Embedding Config

When a semantic field uses a dense model, OpenSearch automatically generates a companion `knn_vector` field to store the embedding. You can use the `dense_embedding_config` parameter to customize how this vector field is configured during index creation.

The structure of `dense_embedding_config` closely mirrors the configuration for a standard `knn_vector` field. You can use this parameter to configure settings like the KNN engine and indexing behavior.

### Restrictions

While `dense_embedding_config` supports most of the same options as knn_vector, there are two key restrictions:

**dimension is not configurable**: The embedding dimension must match the output dimension of the ML model and is automatically inferred from the `model_id`. You should set the correct dimension in the model configuration, not in the field mapping.

**space_type is not configurable**: The similarity space (e.g. cosinesimil, l2, innerproduct) must also align with the model and is resolved from the model configuration.

### How to Configure

To see what values are supported in `dense_embedding_config`, refer to the [KNN vector mapping documentation]({{site.url}}{{site.baseurl}}/field-types/supported-field-types/knn-vector/#parameters). Most fields are valid, except for dimension and space_type, which are model-dependent.

Example;
```json
PUT /my-nlp-index
{
  "mappings": {
    "properties": {
      "passage": {
        "type": "semantic",
        "model_id": "nF7yX5cBsaYnPfyOq2SG",
        "dense_embedding_config": {
          "method": {
            "name": "hnsw",
            "engine": "lucene",
            "parameters": {
              "ef_construction": 128,
              "m": 32
            }
          }
        }
      }
    }
  }
}
```

## Sparse Encoding Config
When a semantic field uses a sparse model, OpenSearch automatically generates a companion field to store the sparse vector representation. By default, this vector is pruned to reduce dimensionality and improve efficiency.

The sparse_encoding_config parameter allows you to control how pruning is applied during encoding by specifying a strategy and its parameters. This provides fine-grained control over how sparse vectors are indexed, balancing accuracy and storage/performance.

The sparse_encoding_config object supports any pruning strategy available in the [sparse encoding ingest processor]({{site.url}}{{site.baseurl}}/ingest-pipelines/processors/sparse-encoding/#pruning-sparse-vectors).

Example:

```json
PUT /my-nlp-index
{
  "mappings": {
    "properties": {
      "passage": {
        "type": "semantic",
        "model_id": "nF7yX5cBsaYnPfyOq2SG",
        "sparse_encoding_config": {
          "prune_type": "top_k",
          "prune_ratio": 64
        }
      }
    }
  }
}
```
This keeps only the 64 highest-scoring terms in the sparse vector.

## Ingest batch size for semantic fields
When documents are ingested into an index with semantic fields, OpenSearch uses a system-generated ingest pipeline to call the underlying ML model and generate embeddings. To optimize performance, these operations are batched.

You can control the number of documents processed together during this step using the index-level setting `index.neural_search.semantic_ingest_batch_size`.

This dynamic index setting specifies the **number of documents** batched together when generating embeddings for semantic fields during ingestion.

| Property | Value                                     |
| -------- | ----------------------------------------- |
| Type     | Integer                                   |
| Default  | `10`                                      |
| Scope    | Dynamic (can be updated on an open index) |

Batching improves throughput by reducing the overhead of model inference. However, increasing the batch size may also increase memory usage. You should tune this setting based on your model's performance characteristics and expected ingest volume.

This setting controls how many documents are processed together in a batch, but it does not directly determine the size of the input sent to the model.
{: .note}

If a single document contains multiple semantic fields, embeddings will be generated for each one.
{: .note}

If text chunking is enabled for a semantic field, the content may be split into multiple chunks, and embeddings will be generated for each chunk.
As a result, the actual number of model inference calls and total embedding inputs per batch may be significantly higher than the batch size value.
{: .note}

Example:

```json
PUT /my-index/_settings
{
  "index": {
    "neural_search.semantic_ingest_batch_size": 32
  }
}
```

This updates the ingest batch size to 32 for the my-index index. The change takes effect immediately and applies to all subsequent ingested documents containing semantic fields.


## Limitations

Note the following limitations of the `semantic` field:

- **Remote cluster support**: Neural queries on `semantic` fields are not supported in cross-cluster search. While documents can be retrieved from remote indexes, semantic queries require access to local model configuration and index mappings. Which means you need to query against the embedding directly using the traditional way.
- **Repeated inference behavior**: When updating a document, OpenSearch will rerun inference for the `semantic` field even if the field's content has not changed. Currently, there is no support for reusing existing embeddings to avoid redundant inference.
- **Mapping constraints**: The `semantic` field does not support dynamic mapping. You must define it explicitly in the index mapping. Additionally, you cannot use a `semantic` field in the `fields` section of another field, meaning that multi-field configurations are not supported.

## Next steps

- [Using a `semantic` field with text embedding models for semantic search]({{site.url}}{{site.baseurl}}/vector-search/ai-search/semantic-search/#using-a-semantic-field)
- [Using a `semantic` field with sparse encoding models for neural sparse search]({{site.url}}{{site.baseurl}}/vector-search/ai-search/neural-sparse-with-pipelines/#using-a-semantic-field)